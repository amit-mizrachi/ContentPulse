# ========================================================================
# INFERENCE SERVICE - HELM VALUES
# ========================================================================
# Configuration values come from Terraform-managed ConfigMap (infra-config)
# Deploy: helm install inference ./charts/llm-judge-service -f releases/inference-values.yaml
# ========================================================================

service:
  name: inference-service
  type: ClusterIP
  containerPort: 8003        # Must match PORT_INFERENCE in ConfigMap
  portKey: PORT_INFERENCE    # ConfigMap key for SERVICE_PORT

image:
  pullPolicy: Always

serviceAccount:
  create: true
  annotations:
    eks.amazonaws.com/role-arn: ""  # Set from Terraform output
  name: inference-service

# Enable SQS worker config loading
sqsWorker: true

resources:
  requests:
    cpu: 500m
    memory: 512Mi
  limits:
    cpu: 2000m
    memory: 2Gi

autoscaling:
  enabled: true
  minReplicas: 2
  maxReplicas: 20

# AppConfig bootstrap IDs - set from Terraform outputs
appConfig:
  applicationId: ""
  environmentId: ""
  profileId: ""

ingress:
  enabled: false

networkPolicy:
  enabled: true
  ingress:
    - podSelector: {}
      ports:
        - port: 8003
          protocol: TCP
  egress:
    - ports:
        - port: 53
          protocol: UDP
    - ipBlock:
        cidr: 0.0.0.0/0
      ports:
        - port: 443
          protocol: TCP
    - podSelector:
        matchLabels:
          app: redis-service
      ports:
        - port: 8001
          protocol: TCP

topologySpreadConstraints:
  - maxSkew: 1
    topologyKey: topology.kubernetes.io/zone
    whenUnsatisfiable: DoNotSchedule
    labelSelector:
      matchLabels:
        app: inference-service

nodeSelector:
  role: application
